
import numpy as np

# --- Scene and Camera Parameters ---
FX = 533.895
FY = 534.07
IMAGE_W = 1280
IMAGE_H = 720
MU = np.array([0.0, 0.0, 0.0]) # ROI center
DIST_MIN = 7 # Minimum allowed distance to ROI center for the agent

# --- Environment Generation ---
# Point Cloud parameters (randomized range)
PCD_A_MIN, PCD_A_MAX = 1.0, 3.5
PCD_B_MIN, PCD_B_MAX = 1.0, 3.5
PCD_C_MIN, PCD_C_MAX = 1.0, 3.5
PCD_N_SAMPLES = 500

# Obstacle generation parameters
NUM_OBSTACLES = 8
OBSTACLE_MIN_DIST_FROM_ROI = 15#25#15
OBSTACLE_MAX_DIST_FROM_ROI = 40#130#40
OBSTACLE_MIN_RADIUS = 2.0#1.0 #2
OBSTACLE_MAX_RADIUS = 5.0 #5

# Agent starting position generation parameters
AGENT_MIN_START_DIST_FROM_ROI = 50#70
AGENT_MAX_START_DIST_FROM_ROI = 80#150 #150, reward 120
ENV_BOUNDS_X = [-100, 100]
ENV_BOUNDS_Y = [-100, 100]


# --- State Representation ---
BINS = 30
HIST_RANGE = [[0, IMAGE_W], [0, IMAGE_H]]


# --- PPO Agent Hyperparameters (for training) ---
POLICY_PATH = "saved_models/ppo_policy_camera_cnn_whole_circle.pth"
N_EPISODES = 2000
MAX_TIMESTEPS = 50
BATCH_UPDATE_TIMESTEP = 2048#1050
LR_ACTOR = 0.0001
LR_CRITIC = 0.001
GAMMA = 0.99
K_EPOCHS = 3
EPS_CLIP = 0.2
ACTION_STD_INIT = 0.2
ACTION_SCALING = 2.0

ACTION_STD_INIT = 0.6               
ACTION_STD_DECAY_RATE = 0.005      
MIN_ACTION_STD = 0.1    

# --- Evaluation Parameters ---
EVAL_N_STEPS = 50
EVAL_GRID_SIZE = 25 # For reward map generation
EVAL_MAP_BOUNDS = [ENV_BOUNDS_X[0], ENV_BOUNDS_X[1], ENV_BOUNDS_Y[0], ENV_BOUNDS_Y[1]]


import numpy as np
import torch
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter

import config
from src.agent import PPO, Memory
from src.agent_rnn import PPO_RNN, MemoryRNN
from src.environment import point_cloud, project_point, in_fov, is_occluded, env_setup
from src.utils import compute_reward_for_training



def main_rnn():
    """Main function to train the PPO agent with an RNN policy."""
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("Using Apple Metal (MPS) for RNN training.")
    else:
        device = torch.device("cpu")
        print("MPS not available. Using CPU for RNN training.")

    writer = SummaryWriter(log_dir="runs/PPO_RNN_Experiment")
    
    # --- PPO Agent with RNN ---
    state_vec_dim = 2
    action_dim = 2
    rnn_hidden_size = 64
    rnn_n_layers = 1
    
    ppo_agent = PPO_RNN(
        state_vec_dim, action_dim, config.LR_ACTOR, config.LR_CRITIC, 
        config.GAMMA, config.K_EPOCHS, config.EPS_CLIP, device, 
        config.ACTION_STD_INIT, config.ACTION_STD_DECAY_RATE, config.MIN_ACTION_STD,
        rnn_hidden_size, rnn_n_layers  
    )
    memory = MemoryRNN()

    # --- Logging ---
    all_rewards = []
    update_step_count = 0

    print("Starting Training with RNN Agent...")
    for episode in range(config.N_EPISODES):
        # --- Environment Setup (same as before) ---
        a = np.random.uniform(config.PCD_A_MIN, config.PCD_A_MAX)
        b = np.random.uniform(config.PCD_B_MIN, config.PCD_B_MAX)
        c = np.random.uniform(config.PCD_C_MIN, config.PCD_C_MAX)
        pcd3d_world = point_cloud(config.PCD_N_SAMPLES, a, b, c) + config.MU.reshape(3, 1)
        pcd2d_world = pcd3d_world[:2, :].T
        all_indices = np.arange(pcd2d_world.shape[0])
        obstacle_list, initial_camera_pos = env_setup()
        
        state_vec = initial_camera_pos
        current_ep_reward = 0
        
        hidden_state = ppo_agent.init_hidden()

        for t in range(config.MAX_TIMESTEPS):
            # --- State Creation (same as before) ---
            cam_center_i = np.append(state_vec, 0.0)
            dist_i = np.array([[1,0,0],[0,1,0],[0,0,0]]) @ (config.MU - cam_center_i)
            norm_dist_i = np.linalg.norm(dist_i);
            if norm_dist_i < 1e-6: continue
            zc_i = dist_i / norm_dist_i
            xc_i_cand = np.cross(zc_i, [0, 0, 1]); norm_xc_i = np.linalg.norm(xc_i_cand)
            if norm_xc_i < 1e-6: continue
            xc_i = xc_i_cand / norm_xc_i; yc_i = np.cross(zc_i, xc_i); R_i = np.vstack([xc_i, yc_i, zc_i])
            pc_i = R_i @ (pcd3d_world - cam_center_i.reshape(3, 1))
            gnt_point_cloud_px_i = project_point(pc_i.T, config.FX, config.FY)
            gnt_point_cloud_px_fov_i, _ = in_fov(gnt_point_cloud_px_i, config.IMAGE_W, config.IMAGE_H)
            visible_indices = all_indices.copy()
            for obs_center_2d, obs_radius, _ in obstacle_list:
                if visible_indices.size == 0: break
                currently_visible_pcd = pcd2d_world[visible_indices]
                newly_occluded_local_indices = is_occluded(currently_visible_pcd, cam_center_i[:2], obs_center_2d, obs_radius)
                if newly_occluded_local_indices.size > 0:
                    newly_occluded_global_indices = visible_indices[newly_occluded_local_indices]
                    visible_indices = np.setdiff1d(visible_indices, newly_occluded_global_indices, assume_unique=True)
            observed_pc_camera_i = pc_i[:, visible_indices]
            observed_point_cloud_px_i = project_point(observed_pc_camera_i.T, config.FX, config.FY)
            observed_point_cloud_px_fov_i, _ = in_fov(observed_point_cloud_px_i, config.IMAGE_W, config.IMAGE_H)
            H_obs, _, _ = np.histogram2d(observed_point_cloud_px_fov_i[:, 0], observed_point_cloud_px_fov_i[:, 1], bins=config.BINS, range=config.HIST_RANGE)
            H_gnt, _, _ = np.histogram2d(gnt_point_cloud_px_fov_i[:, 0], gnt_point_cloud_px_fov_i[:, 1], bins=config.BINS, range=config.HIST_RANGE)
            state_img = torch.FloatTensor(H_obs.T).unsqueeze(0).unsqueeze(0).to(device)
            state_vec_tensor = torch.FloatTensor(state_vec).unsqueeze(0).to(device)

            h_in, c_in = hidden_state[0].detach(), hidden_state[1].detach()
            action, log_prob, hidden_state = ppo_agent.act(state_img, state_vec_tensor, hidden_state)
            
            state_vec = state_vec + action.cpu().numpy().flatten() * config.ACTION_SCALING

            crashed = False; distance_to_obs = float('inf')
            for obs_center_2d, obs_radius, _ in obstacle_list:
                current_dist_to_obs = np.linalg.norm(state_vec - obs_center_2d)
                distance_to_obs = min(distance_to_obs, current_dist_to_obs)
                if current_dist_to_obs < obs_radius: crashed = True; break
            distance_to_roi = np.linalg.norm(state_vec - config.MU[:2])
            
            reward, _ = compute_reward_for_training(H_gnt.flatten(), H_obs.flatten(), distance_to_roi, config.DIST_MIN, distance_to_obs)
            
            
            
            if crashed:
                reward = -3000
                print(f"Episode {episode+1} crashed at timestep {t+1}. Reward: {reward:.4f}")
                done = True
            else:
                done = t == config.MAX_TIMESTEPS - 1


            memory.store_timestep(action, state_img, state_vec_tensor, log_prob, reward, done, h_in, c_in)
            
            current_ep_reward += reward
            if done: break
        
        memory.finish_trajectory()
        avg_episode_reward = current_ep_reward / (t + 1)
        all_rewards.append(avg_episode_reward)
        writer.add_scalar('Reward/Average_Episode_Reward_RNN', avg_episode_reward, episode)
        writer.add_scalar('Metrics/Final_Distance_to_ROI_RNN', distance_to_roi, episode)
        print(f"Episode {episode+1}/{config.N_EPISODES}: Avg Reward = {all_rewards[-1]:.4f}, Total Timesteps in Mem: {len(memory)}")

        # --- Update Policy ---
        if len(memory) >= config.BATCH_UPDATE_TIMESTEP:
            print(f"Updating PPO-RNN agent... at update step: {update_step_count}")
            p_loss, v_loss, entropy, avg_value = ppo_agent.update(memory)
            ppo_agent.decay_action_std()
            writer.add_scalar('Loss/Policy_Loss_RNN', p_loss, update_step_count)
            writer.add_scalar('Loss/Value_Loss_RNN', v_loss, update_step_count)
            writer.add_scalar('Metrics/Policy_Entropy_RNN', entropy, update_step_count)
            update_step_count += 1
            memory.clear()

    print("Training finished.")
    rnn_policy_path = config.POLICY_PATH.replace('.pth', '_rnn.pth')
    torch.save(ppo_agent.policy.state_dict(), rnn_policy_path)
    print(f"RNN Policy saved to {rnn_policy_path}")
    writer.close()
    
    # --- Visualization (add your plots here if needed) ---
    plt.figure(figsize=(10, 5))
    plt.plot(all_rewards)
    plt.xlabel("Episode")
    plt.ylabel("Average Reward per Episode")
    plt.title("RNN Training Progress")
    plt.grid(True)
    plt.savefig("rnn_training_rewards.png")
    plt.show()
    
    
def main():
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("Using Apple Metal (MPS) for training.")
    else:
        device = torch.device("cpu")
        print("MPS not available. Using CPU for training.")


    writer = SummaryWriter()
    
    
    # --- PPO Agent ---
    state_vec_dim = 2 # (x, y) position
    action_dim = 2
    ppo_agent = PPO(state_vec_dim, action_dim, config.LR_ACTOR, config.LR_CRITIC, 
                    config.GAMMA, config.K_EPOCHS, config.EPS_CLIP, device, config.ACTION_STD_INIT, config.ACTION_STD_DECAY_RATE, config.MIN_ACTION_STD)
    memory = Memory()

    # --- Logging ---
    all_rewards = []
    all_cam_positions = []
    all_mse_loss = []
    all_start_pos = []
    all_end_pos = []
    illegal_start_pos = []
    all_dist_to_roi = []
    all_dist_to_obs = []
    illegal_start_obs_pos = []
    
    timestep = 0
    update_step_count = 0

    print("Starting Training...")
    for episode in range(config.N_EPISODES):
        # Create a new random environment for each episode
        a = np.random.uniform(config.PCD_A_MIN, config.PCD_A_MAX)
        b = np.random.uniform(config.PCD_B_MIN, config.PCD_B_MAX)
        c = np.random.uniform(config.PCD_C_MIN, config.PCD_C_MAX)
        pcd3d_world = point_cloud(config.PCD_N_SAMPLES, a, b, c) + config.MU.reshape(3, 1)
        pcd2d_world = pcd3d_world[:2, :].T
        all_indices = np.arange(pcd2d_world.shape[0])
        
        obstacle_list, initial_camera_pos = env_setup()
        all_start_pos.append(initial_camera_pos)
        
        state_vec = initial_camera_pos
        current_ep_reward = 0

        for t in range(config.MAX_TIMESTEPS):
            timestep += 1
            
            cam_center_i = np.append(state_vec, 0.0)

            # --- Camera Orientation ---
            dist_i = np.array([[1,0,0],[0,1,0],[0,0,0]]) @ (config.MU - cam_center_i)
            norm_dist_i = np.linalg.norm(dist_i)
            if norm_dist_i < 1e-6: continue
            zc_i = dist_i / norm_dist_i
            xc_i_cand = np.cross(zc_i, [0, 0, 1])
            norm_xc_i = np.linalg.norm(xc_i_cand)
            if norm_xc_i < 1e-6: continue
            xc_i = xc_i_cand / norm_xc_i
            yc_i = np.cross(zc_i, xc_i)
            R_i = np.vstack([xc_i, yc_i, zc_i])
            
            # --- Point Cloud Projection and Occlusion ---
            pc_i = R_i @ (pcd3d_world - cam_center_i.reshape(3, 1))
            gnt_point_cloud_px_i = project_point(pc_i.T, config.FX, config.FY)
            gnt_point_cloud_px_fov_i, _ = in_fov(gnt_point_cloud_px_i, config.IMAGE_W, config.IMAGE_H)

            # still_visible_indices = all_indices.copy()
            # for obs_center_2d, obs_radius, _ in obstacle_list:
            #     occluded_by_obs = is_occluded(pcd2d_world, cam_center_i[:2], obs_center_2d, obs_radius)
            #     still_visible_indices = np.setdiff1d(still_visible_indices, occluded_by_obs, assume_unique=True)
            
            
            visible_indices = all_indices.copy()

            for obs_center_2d, obs_radius, _ in obstacle_list:
                if visible_indices.size == 0:
                    break

                currently_visible_pcd = pcd2d_world[visible_indices]

                newly_occluded_local_indices = is_occluded(
                    currently_visible_pcd, cam_center_i[:2], obs_center_2d, obs_radius
                )

                if newly_occluded_local_indices.size > 0:
                    newly_occluded_global_indices = visible_indices[newly_occluded_local_indices]

                    
                    visible_indices = np.setdiff1d(
                        visible_indices, newly_occluded_global_indices, assume_unique=True
                    )

            still_visible_indices = visible_indices

            observed_pc_camera_i = pc_i[:, still_visible_indices]
            observed_point_cloud_px_i = project_point(observed_pc_camera_i.T, config.FX, config.FY)
            observed_point_cloud_px_fov_i, _ = in_fov(observed_point_cloud_px_i, config.IMAGE_W, config.IMAGE_H)
            
            # --- State Creation ---
            H_obs, _, _ = np.histogram2d(
                observed_point_cloud_px_fov_i[:, 0], observed_point_cloud_px_fov_i[:, 1], 
                bins=config.BINS, range=config.HIST_RANGE
            )
            H_gnt, _, _ = np.histogram2d(
                gnt_point_cloud_px_fov_i[:, 0], gnt_point_cloud_px_fov_i[:, 1], 
                bins=config.BINS, range=config.HIST_RANGE
            )
            
            state_img = torch.FloatTensor(H_obs.T).unsqueeze(0).unsqueeze(0).to(device)
            state_vec_tensor = torch.FloatTensor(state_vec).unsqueeze(0).to(device)

            # --- Agent Action ---
            action, log_prob = ppo_agent.act(state_img, state_vec_tensor)
            action_np = action.cpu().numpy().flatten()
            state_vec = state_vec + action_np * config.ACTION_SCALING

            # --- Crash Detection and Reward ---
            crashed = False
            min_dist_obs = float('inf')
            for obs_center_2d, obs_radius, _ in obstacle_list:
                distance_to_obs = np.linalg.norm(state_vec - obs_center_2d)
                

                if distance_to_obs < min_dist_obs:
                    min_dist_obs = distance_to_obs
                    
                if distance_to_obs < obs_radius:
                    crashed = True
                    break
            distance_to_obs = min_dist_obs if np.isfinite(min_dist_obs) else 0.0
            
            distance_to_roi = np.linalg.norm(state_vec - config.MU[:2])
            reward, ratio = compute_reward_for_training(H_gnt.flatten(), H_obs.flatten(), distance_to_roi, config.DIST_MIN, distance_to_obs)
            
            if crashed:
                reward = -3000
                print(f"Episode {episode+1} crashed at timestep {t+1}. Reward: {reward:.4f}")
                done = True
            else:
                done = t == config.MAX_TIMESTEPS - 1

            # --- Store and Update ---
            memory.states_img.append(state_img)
            memory.states_vec.append(state_vec_tensor)
            memory.actions.append(action)
            memory.logprobs.append(log_prob)
            memory.rewards.append(reward)
            memory.is_terminals.append(done)

            # if timestep % config.BATCH_UPDATE_TIMESTEP == 0:
            #     # Update the PPO agent
            #     print("Updating PPO agent... at timestep:", timestep)
            #     ppo_agent.update(memory)
                
            #     memory.clear()
            #     timestep = 0
            
            if timestep % config.BATCH_UPDATE_TIMESTEP == 0:
                print(f"Updating PPO agent... at update step: {update_step_count}")
                p_loss, v_loss, entropy, avg_value = ppo_agent.update(memory)
                
                ppo_agent.decay_action_std()
                
                # --- LOG METRICS TO TENSORBOARD ---
                writer.add_scalar('Loss/Policy_Loss', p_loss, update_step_count)
                writer.add_scalar('Loss/Value_Loss', v_loss, update_step_count)
                writer.add_scalar('Metrics/Policy_Entropy', entropy, update_step_count)
                writer.add_scalar('Metrics/Average_Critic_Value', avg_value, update_step_count)
                writer.add_scalar('Metrics/Action_STD', ppo_agent.action_std, update_step_count)

                
                update_step_count += 1
                memory.clear()
                
                timestep = 0

                
                
            current_ep_reward += reward
            all_cam_positions.append(state_vec.copy())


            # if done:
            #     all_end_pos.append(state_vec.copy())
            #     break
            if done or crashed:
                all_end_pos.append(state_vec.copy())
                break
                
        avg_episode_reward = current_ep_reward / (t + 1)
        all_rewards.append(avg_episode_reward)
        all_dist_to_roi.append(distance_to_roi)

        # --- LOG EPISODE-LEVEL METRICS ---
        writer.add_scalar('Reward/Average_Episode_Reward', avg_episode_reward, episode)
        writer.add_scalar('Metrics/Final_Distance_to_ROI', distance_to_roi, episode)
        
        # all_rewards.append(current_ep_reward / config.MAX_TIMESTEPS)
        # all_dist_to_roi.append(distance_to_roi)
        print(f"Episode {episode+1}/{config.N_EPISODES}: Avg Reward = {all_rewards[-1]:.4f}")

    print("Training finished.")
    torch.save(ppo_agent.policy.state_dict(), config.POLICY_PATH)
    print(f"Policy saved to {config.POLICY_PATH}")
    
    writer.close()

    # --- Visualization ---
    plt.figure(figsize=(10, 5))
    plt.plot(all_rewards)
    plt.xlabel("Episode")
    plt.ylabel("Average Reward per Episode")
    plt.title("Training Progress")
    plt.grid(True)
    # plt.show()
    
    # --- Visualization ---
    cam_centers = np.array(all_cam_positions)
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))

    ax1.plot(all_rewards, c='b', label="Average reward per episode")
    ax1.set_xlabel("Episode")
    ax1.set_ylabel("Reward")
    ax1.set_title("Rewards")
    ax1.legend()
    ax1.grid(True)


    num_steps_total = cam_centers.shape[0]
    step_indices = np.arange(num_steps_total)
    all_start_pos_np = np.array(all_start_pos)
    all_end_pos_np = np.array(all_end_pos)

    sc = ax2.scatter(cam_centers[:, 0], cam_centers[:, 1], c=step_indices, s=10, alpha=0.7, label='Camera centers')
    ax2.scatter(pcd3d_world[0,:], pcd3d_world[1,:], label='Point cloud (last ep.)')

    for obs_pos_2d, obs_radius, obs_id in obstacle_list:
        ax2.add_patch(plt.Circle(obs_pos_2d, obs_radius, fill=False, color='r', linewidth=2))
        ax2.text(obs_pos_2d[0], obs_pos_2d[1], str(obs_id), color='r', fontsize=12, ha='center', va='center')

    ax2.plot([], [], color='r', linewidth=2, label='Obstacles (last ep.)')[0].set_visible(False)
    ax2.scatter(config.MU[0], config.MU[1], c='b', marker='x', s=150, label='ROI center')
    ax2.scatter(all_start_pos_np[:,0], all_start_pos_np[:,1], c='g', label='Start')
    ax2.scatter(all_end_pos_np[:,0], all_end_pos_np[:,1], c='r', label="End")

    ax2.set_xlabel("X")
    ax2.set_ylabel("Y")
    ax2.set_title("Camera Trajectories Over Training")
    ax2.set_aspect('equal')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    cbar = plt.colorbar(sc, ax=ax2)
    cbar.set_label("Timestep")
    fig.savefig("trajectories_and_reward.png")

    fig2, (ax3, ax4) = plt.subplots(1, 2, figsize=(16, 7))
    ax3.plot(all_dist_to_obs)
    ax3.set_ylabel("Distance")
    ax3.set_xlabel("Episode")
    ax3.set_title("Final Distance to Nearest Obstacle")
    ax3.grid(True)

    ax4.plot(all_dist_to_roi)
    ax4.set_ylabel("Distance")
    ax4.set_xlabel("Episode")
    ax4.set_title("Final Distance to ROI")
    ax4.grid(True)

    plt.tight_layout()
    fig2.savefig("distances_over_episodes.png") 

    plt.show()

if __name__ == "__main__":
    main()
    # main_rnn()


import numpy as np

def compute_ratio(H_gnt, H_obs):
    """Computes the ratio of observed intensity to ground truth intensity."""
    sum_gnt = np.sum(H_gnt)
    sum_obs = np.sum(H_obs)
    if sum_gnt < 1e-9:
        return 0.0 # Avoid division by zero
    return sum_obs / sum_gnt

def compute_non_seen_ratio(H_obs, H_gnt):
    """Computes the ratio of non-seen intensity."""
    sum_gnt = np.sum(H_gnt)
    sum_obs = np.sum(H_obs)
    if sum_gnt < 0:
        return 1.0 # If nothing should be seen, then 100% is non-seen
    return (sum_gnt - sum_obs) / sum_gnt

def compute_entropy(H_obs):
    """Computes the sum of intensities in the observed histogram as a proxy for entropy."""
    
    return np.sum(H_obs)

def check_min_distance(dist_to_roi, d_min):
    """Checks if the agent is closer than the minimum allowed distance."""
    if dist_to_roi < d_min:
        traversed_dist = dist_to_roi
        return 1, traversed_dist # Returns 1 (penalty enabled) and the distance inside the zone
    else:
        return 0, 1 # Returns 0 (no penalty) and 0 distance

def compute_reward(H_gnt, H_obs, dist_to_roi, d_min):
    """
    Computes the reward for the current state.
    """
    if np.sum(H_gnt) < 1e-9:
        return 0.0, 0.0
    
    non_seen_ratio = compute_non_seen_ratio(H_obs,H_gnt)
    entropy = compute_entropy(H_obs)
    enable_min_dist, traversed_distance = check_min_distance(dist_to_roi,d_min)

    ratio = np.sum(H_obs) / np.sum(H_gnt)
    # reward = ratio
    max_o = np.max(H_obs)
    max_g = np.max(H_gnt)
    
    if max_o <= 0: max_o = 1.0
    
    if max_g <= 0: max_g = 1.0

    
    # reward = 2 * entropy  - 400 * non_seen_ratio - 0 * dist_to_roi - 1 * (enable_min_dist*traversed_distance)
    # reward = 0.5 * entropy - 2000 * non_seen_ratio - 400 * (enable_min_dist*traversed_distance) - 50 * dist_to_roi
    # reward = 0.5 * entropy - 10 * non_seen_ratio - 400 * (enable_min_dist*traversed_distance) - 10 * dist_to_roi
    # this is the old that we liked, and seemed to be working on dimitris office
    
    #current entropy is computed - unnormalized, therefore i am replication the mistake by dividing with max_obs()
    
    reward = (0.5 * entropy) / max_o - 240 * non_seen_ratio - 400 * enable_min_dist  #THIS IS THE ONE WE ARE WORKING WITH DESPOINA
    # reward = -400 * ( enable_min_dist/traversed_distance)  # - 0 * non_seen_ratio - 0 * dist_to_roi - 400 * enable_min_dist  
    # reward = - 500 * dist_to_roi
    
    return reward, ratio

def compute_reward_for_training(H_gnt, H_obs, dist_to_roi, d_min,distance_to_obs):
    """
    Computes the reward using the formula from your training script.
    It includes penalties for being too close and for non-seen regions.
    """
    # entropy = compute_entropy(H_obs)
    # non_seen_ratio = compute_non_seen_ratio(H_obs, H_gnt)
    # enable_min_dist, traversed_distance = check_min_distance(dist_to_roi, d_min)

    # # From train_ppo_cnn_whole_experiment.py:
    # # reward = 0.5 * entropy - 20 * non_seen_ratio - 0 * dist_to_roi - 400 * enable_min_dist
    # # reward = 0.5 * entropy - 20 * non_seen_ratio - 400 * (enable_min_dist*traversed_distance) - 10 * dist_to_roi
    
    # # reward = 0.5 * entropy - 10 * non_seen_ratio - 400 * (enable_min_dist*traversed_distance) - 10 * dist_to_roi

    # # reward = -500 * dist_to_roi
    
    
    # max_o = np.max(H_obs)
    # max_g = np.max(H_gnt)
    
    # if max_o <= 0: max_o = 1.0
    
    # if max_g <= 0: max_g = 1.0
        
    # ratio = compute_ratio(H_gnt, H_obs)
    # # reward = (0.5 * entropy) / max_o - 120 * non_seen_ratio - 400 * enable_min_dist  #THIS IS THE ONE WE ARE WORKING WITH DESPOINA

    # # reward = (0.5 * entropy) / max_o - 240 * non_seen_ratio - 400 * enable_min_dist  #THIS IS THE ONE WE ARE WORKING WITH DESPOINA
    
    # reward = ratio - 0.5 * dist_to_roi - 400 * enable_min_dist 
    
    

    ratio = compute_ratio(H_gnt, H_obs)
    visibility_reward = 1800 * ratio

    distance_penalty = - 1/4 * dist_to_roi**2

  
    enable_min_dist, traversed_distance = check_min_distance(dist_to_roi, d_min)
    too_close_penalty = -400 * (enable_min_dist * (d_min - traversed_distance))

    timestep_penalty = -1
    
    success_bonus = 0
    if ratio > 0.9:
        success_bonus = 500
        
    occlusion_penalty = 0
    if ratio < 0.9:
        occlusion_penalty = -500
        
    occlusion_penalty = -3500 * (1 - ratio)

    # dist_to_obs_penalty = -1000 * distance_to_obs**2
    
    if distance_to_obs < 5: #was 7
        dist_to_obs_penalty = -4000#- 60 *  distance_to_obs**2
    else:
        dist_to_obs_penalty = 0
        
    
        
    reward =  distance_penalty + too_close_penalty + timestep_penalty  + occlusion_penalty + dist_to_obs_penalty + success_bonus

    return reward, ratio

