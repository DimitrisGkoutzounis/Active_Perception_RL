TESTING 0.99



import numpy as np
import config
import json
import os



def save_config_to_json(writer_log_dir):
    config_dict = {}
    
    for attr_name in dir(config):
        if not attr_name.startswith('_'):  
            attr_value = getattr(config, attr_name)
            if isinstance(attr_value, (int, float, str, bool, list, tuple)):
                config_dict[attr_name] = attr_value
            elif isinstance(attr_value, np.ndarray):
                config_dict[attr_name] = attr_value.tolist()
    
    config_path = os.path.join(writer_log_dir, 'config.json')
    
    with open(config_path, 'w') as f:
        json.dump(config_dict, f, indent=2)
    
    print(f"Config saved to: {config_path}")
    
    
    
# def compute_nearest_obstacle_distance(agent_pos, obstacle_list):

def compute_ratio(H_gnt, H_obs):
    """Computes the ratio of observed intensity to ground truth intensity."""
    sum_gnt = np.sum(H_gnt)
    sum_obs = np.sum(H_obs)
    if sum_gnt < 1e-9:
        return 0.0 # Avoid division by zero
    return sum_obs / sum_gnt

def compute_non_seen_ratio(H_obs, H_gnt):
    """Computes the ratio of non-seen intensity."""
    sum_gnt = np.sum(H_gnt)
    sum_obs = np.sum(H_obs)
    if sum_gnt < 0:
        return 1.0 # If nothing should be seen, then 100% is non-seen
    return (sum_gnt - sum_obs) / sum_gnt

def compute_entropy(H_obs):
    """Computes the sum of intensities in the observed histogram as a proxy for entropy."""
    
    return np.sum(H_obs)

def check_min_distance(dist_to_roi, d_min):
    """Checks if the agent is closer than the minimum allowed distance."""
    if dist_to_roi < d_min:
        traversed_dist = dist_to_roi
        return 1, traversed_dist 
    else:
        return 0, 1 # Returns 0 (no penalty) and 0 distance

def compute_reward(H_gnt, H_obs, dist_to_roi, d_min):
    """
    Computes the reward for the current state.
    """
    if np.sum(H_gnt) < 1e-9:
        return 0.0, 0.0
    
    non_seen_ratio = compute_non_seen_ratio(H_obs,H_gnt)
    entropy = compute_entropy(H_obs)
    enable_min_dist, traversed_distance = check_min_distance(dist_to_roi,d_min)

    ratio = np.sum(H_obs) / np.sum(H_gnt)
    # reward = ratio
    max_o = np.max(H_obs)
    max_g = np.max(H_gnt)
    
    if max_o <= 0: max_o = 1.0
    
    if max_g <= 0: max_g = 1.0

    
    # reward = 2 * entropy  - 400 * non_seen_ratio - 0 * dist_to_roi - 1 * (enable_min_dist*traversed_distance)
    # reward = 0.5 * entropy - 2000 * non_seen_ratio - 400 * (enable_min_dist*traversed_distance) - 50 * dist_to_roi
    # reward = 0.5 * entropy - 10 * non_seen_ratio - 400 * (enable_min_dist*traversed_distance) - 10 * dist_to_roi
    # this is the old that we liked, and seemed to be working on dimitris office
    
    #current entropy is computed - unnormalized, therefore i am replication the mistake by dividing with max_obs()
    
    reward = (0.5 * entropy) / max_o - 240 * non_seen_ratio - 400 * enable_min_dist  #THIS IS THE ONE WE ARE WORKING WITH DESPOINA
    # reward = -400 * ( enable_min_dist/traversed_distance)  # - 0 * non_seen_ratio - 0 * dist_to_roi - 400 * enable_min_dist  
    # reward = - 500 * dist_to_roi
    
    return reward, ratio

def compute_reward_for_training(H_gnt, H_obs, dist_to_roi, d_min,distance_to_obs):
    """
    Computes the reward using the formula from your training script.
    It includes penalties for being too close and for non-seen regions.
    """
    # entropy = compute_entropy(H_obs)
    # non_seen_ratio = compute_non_seen_ratio(H_obs, H_gnt)
    # enable_min_dist, traversed_distance = check_min_distance(dist_to_roi, d_min)

    # # From train_ppo_cnn_whole_experiment.py:
    # # reward = 0.5 * entropy - 20 * non_seen_ratio - 0 * dist_to_roi - 400 * enable_min_dist
    # # reward = 0.5 * entropy - 20 * non_seen_ratio - 400 * (enable_min_dist*traversed_distance) - 10 * dist_to_roi
    
    # # reward = 0.5 * entropy - 10 * non_seen_ratio - 400 * (enable_min_dist*traversed_distance) - 10 * dist_to_roi

    # # reward = -500 * dist_to_roi
    
    
    # max_o = np.max(H_obs)
    # max_g = np.max(H_gnt)
    
    # if max_o <= 0: max_o = 1.0
    
    # if max_g <= 0: max_g = 1.0
        
    # ratio = compute_ratio(H_gnt, H_obs)
    # # reward = (0.5 * entropy) / max_o - 120 * non_seen_ratio - 400 * enable_min_dist  #THIS IS THE ONE WE ARE WORKING WITH DESPOINA

    # # reward = (0.5 * entropy) / max_o - 240 * non_seen_ratio - 400 * enable_min_dist  #THIS IS THE ONE WE ARE WORKING WITH DESPOINA
    
    # reward = ratio - 0.5 * dist_to_roi - 400 * enable_min_dist 
    
    

    ratio = compute_ratio(H_gnt, H_obs)
    visibility_reward = 6800 * ratio

    non_seen_ratio = compute_non_seen_ratio(H_obs,H_gnt)
    
    distance_penalty = - 1/4 * dist_to_roi**2

  
    enable_min_dist, traversed_distance = check_min_distance(dist_to_roi, d_min)
    too_close_penalty = -400 * (enable_min_dist * (d_min - traversed_distance))

    timestep_penalty = -1
    
    success_bonus = 0
    if ratio > 0.85:
        success_bonus = 1000
        
    occlusion_penalty = 0
    if ratio < 0.25:
        occlusion_penalty = -500
        
    # occlusion_penalty = -3500 * (1 - ratio)

    # dist_to_obs_penalty = -1000 * distance_to_obs**2
    
    if distance_to_obs < 5: #was 7
        dist_to_obs_penalty = -4000#- 60 *  distance_to_obs**2
    else:
        dist_to_obs_penalty = 0
        
    
        
    reward =  distance_penalty + too_close_penalty + timestep_penalty  + dist_to_obs_penalty + success_bonus + occlusion_penalty

    return reward, ratio

